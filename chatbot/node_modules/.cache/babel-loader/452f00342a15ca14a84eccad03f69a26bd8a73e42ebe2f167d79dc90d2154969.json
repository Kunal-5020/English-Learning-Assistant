{"ast":null,"code":"var _s = $RefreshSig$();\n// src/useVoiceRecognition.js\n\nimport { useState, useEffect } from 'react';\nconst useVoiceRecognition = () => {\n  _s();\n  const [transcript, setTranscript] = useState('');\n  const [recognition, setRecognition] = useState(null);\n  useEffect(() => {\n    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n    if (SpeechRecognition) {\n      const recognitionInstance = new SpeechRecognition();\n      recognitionInstance.interimResults = true; // Get interim results\n      recognitionInstance.lang = 'en-US'; // Set language\n\n      recognitionInstance.onresult = event => {\n        const currentTranscript = Array.from(event.results).map(result => result[0].transcript).join('');\n        setTranscript(currentTranscript);\n      };\n      recognitionInstance.onerror = event => {\n        console.error('Speech recognition error detected: ' + event.error);\n      };\n      recognitionInstance.onend = () => {\n        setTimeout(() => {\n          recognitionInstance.start();\n        }, 1000); // Restart after 1 second\n      };\n      setRecognition(recognitionInstance);\n    } else {\n      console.error('Speech recognition not supported in this browser.');\n    }\n    return () => {\n      if (recognition) {\n        recognition.stop();\n      }\n    };\n  }, []);\n  const startListening = () => {\n    if (recognition && recognition.state !== 'running') {\n      setTranscript(''); // Clear previous transcript\n      recognition.start();\n    }\n  };\n  const stopListening = () => {\n    if (recognition) {\n      recognition.stop();\n    }\n  };\n  return {\n    transcript,\n    startListening,\n    stopListening\n  };\n};\n_s(useVoiceRecognition, \"D0AebQ+YIelTB+4fcC8x16e0LLk=\");\nexport default useVoiceRecognition;","map":{"version":3,"names":["useState","useEffect","useVoiceRecognition","_s","transcript","setTranscript","recognition","setRecognition","SpeechRecognition","window","webkitSpeechRecognition","recognitionInstance","interimResults","lang","onresult","event","currentTranscript","Array","from","results","map","result","join","onerror","console","error","onend","setTimeout","start","stop","startListening","state","stopListening"],"sources":["C:/Users/kunal/OneDrive/Desktop/office work/Hindi Chat Bot/chatbot/src/voice/voice-to-text.js"],"sourcesContent":["// src/useVoiceRecognition.js\r\n\r\nimport { useState, useEffect } from 'react';\r\n\r\nconst useVoiceRecognition = () => {\r\n    const [transcript, setTranscript] = useState('');\r\n    const [recognition, setRecognition] = useState(null);\r\n\r\n    useEffect(() => {\r\n        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\r\n        if (SpeechRecognition) {\r\n            const recognitionInstance = new SpeechRecognition();\r\n            recognitionInstance.interimResults = true; // Get interim results\r\n            recognitionInstance.lang = 'en-US'; // Set language\r\n\r\n            recognitionInstance.onresult = (event) => {\r\n                const currentTranscript = Array.from(event.results)\r\n                    .map(result => result[0].transcript)\r\n                    .join('');\r\n                setTranscript(currentTranscript);\r\n            };\r\n\r\n            recognitionInstance.onerror = (event) => {\r\n                console.error('Speech recognition error detected: ' + event.error);\r\n            };\r\n\r\n            recognitionInstance.onend = () => {\r\n                setTimeout(() => {\r\n                    recognitionInstance.start();\r\n                }, 1000); // Restart after 1 second\r\n            };\r\n\r\n            setRecognition(recognitionInstance);\r\n        } else {\r\n            console.error('Speech recognition not supported in this browser.');\r\n        }\r\n\r\n        return () => {\r\n            if (recognition) {\r\n                recognition.stop();\r\n            }\r\n        };\r\n    },[]);\r\n\r\n    const startListening = () => {\r\n        if (recognition && recognition.state !== 'running') {\r\n            setTranscript(''); // Clear previous transcript\r\n            recognition.start();\r\n        }\r\n    };\r\n\r\n    const stopListening = () => {\r\n        if (recognition) {\r\n            recognition.stop();\r\n        }\r\n    };\r\n\r\n    return { transcript, startListening, stopListening };\r\n};\r\n\r\nexport default useVoiceRecognition;"],"mappings":";AAAA;;AAEA,SAASA,QAAQ,EAAEC,SAAS,QAAQ,OAAO;AAE3C,MAAMC,mBAAmB,GAAGA,CAAA,KAAM;EAAAC,EAAA;EAC9B,MAAM,CAACC,UAAU,EAAEC,aAAa,CAAC,GAAGL,QAAQ,CAAC,EAAE,CAAC;EAChD,MAAM,CAACM,WAAW,EAAEC,cAAc,CAAC,GAAGP,QAAQ,CAAC,IAAI,CAAC;EAEpDC,SAAS,CAAC,MAAM;IACZ,MAAMO,iBAAiB,GAAGC,MAAM,CAACD,iBAAiB,IAAIC,MAAM,CAACC,uBAAuB;IACpF,IAAIF,iBAAiB,EAAE;MACnB,MAAMG,mBAAmB,GAAG,IAAIH,iBAAiB,CAAC,CAAC;MACnDG,mBAAmB,CAACC,cAAc,GAAG,IAAI,CAAC,CAAC;MAC3CD,mBAAmB,CAACE,IAAI,GAAG,OAAO,CAAC,CAAC;;MAEpCF,mBAAmB,CAACG,QAAQ,GAAIC,KAAK,IAAK;QACtC,MAAMC,iBAAiB,GAAGC,KAAK,CAACC,IAAI,CAACH,KAAK,CAACI,OAAO,CAAC,CAC9CC,GAAG,CAACC,MAAM,IAAIA,MAAM,CAAC,CAAC,CAAC,CAACjB,UAAU,CAAC,CACnCkB,IAAI,CAAC,EAAE,CAAC;QACbjB,aAAa,CAACW,iBAAiB,CAAC;MACpC,CAAC;MAEDL,mBAAmB,CAACY,OAAO,GAAIR,KAAK,IAAK;QACrCS,OAAO,CAACC,KAAK,CAAC,qCAAqC,GAAGV,KAAK,CAACU,KAAK,CAAC;MACtE,CAAC;MAEDd,mBAAmB,CAACe,KAAK,GAAG,MAAM;QAC9BC,UAAU,CAAC,MAAM;UACbhB,mBAAmB,CAACiB,KAAK,CAAC,CAAC;QAC/B,CAAC,EAAE,IAAI,CAAC,CAAC,CAAC;MACd,CAAC;MAEDrB,cAAc,CAACI,mBAAmB,CAAC;IACvC,CAAC,MAAM;MACHa,OAAO,CAACC,KAAK,CAAC,mDAAmD,CAAC;IACtE;IAEA,OAAO,MAAM;MACT,IAAInB,WAAW,EAAE;QACbA,WAAW,CAACuB,IAAI,CAAC,CAAC;MACtB;IACJ,CAAC;EACL,CAAC,EAAC,EAAE,CAAC;EAEL,MAAMC,cAAc,GAAGA,CAAA,KAAM;IACzB,IAAIxB,WAAW,IAAIA,WAAW,CAACyB,KAAK,KAAK,SAAS,EAAE;MAChD1B,aAAa,CAAC,EAAE,CAAC,CAAC,CAAC;MACnBC,WAAW,CAACsB,KAAK,CAAC,CAAC;IACvB;EACJ,CAAC;EAED,MAAMI,aAAa,GAAGA,CAAA,KAAM;IACxB,IAAI1B,WAAW,EAAE;MACbA,WAAW,CAACuB,IAAI,CAAC,CAAC;IACtB;EACJ,CAAC;EAED,OAAO;IAAEzB,UAAU;IAAE0B,cAAc;IAAEE;EAAc,CAAC;AACxD,CAAC;AAAC7B,EAAA,CAtDID,mBAAmB;AAwDzB,eAAeA,mBAAmB","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}